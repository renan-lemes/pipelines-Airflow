# üå¨Ô∏è Airflow MySQL ‚Üî BigQuery ou BigQuery ‚Üî MySQL Pipelines

Este reposit√≥rio cont√©m pipelines desenvolvidos em **Apache Airflow** para **extrair, transformar e carregar (ETL)** dados entre **MySQL** e **Google BigQuery**.  
Os fluxos permitem tanto a transfer√™ncia de dados do MySQL para o BigQuery quanto o caminho inverso, garantindo **automa√ß√£o, escalabilidade e controle** em todo o processo e focando principalmente na orquestra√ß√£o de dados.

---

## üéØ Objetivo

Substituir processos manuais ou scripts isolados por **pipelines orquestrados** com o Airflow, oferecendo:

- üîÑ **Automa√ß√£o e flexibilidade** no agendamento e controle de fluxos.  
- üìà **Escalabilidade e monitoramento** centralizado.  
- üõ†Ô∏è **Facilidade de manuten√ß√£o, reuso e versionamento**.  

---

## ‚öôÔ∏è Funcionalidades Principais

- Extra√ß√£o de dados a partir do **MySQL**.  
- Carga de dados no **BigQuery** (e vice-versa).  
- Transforma√ß√µes leves em **Python**.  
- Execu√ß√µes **agendadas e autom√°ticas** via Airflow.  
- Monitoramento detalhado via **Airflow UI**.  
- **Envio autom√°tico de e-mails (Gmail)** em caso de falhas nas pipelines.  
- Preparado para **integra√ß√£o CI/CD**, permitindo versionamento e implanta√ß√£o cont√≠nua.  

---

## üß© Pipelines Dispon√≠veis

| Pipeline | Descri√ß√£o |
|-----------|------------|
| **`bigquery_to_mysql_only`** | Carrega dados crus do **BigQuery** para o **MySQL**. |
| **`list_bigquery_tables_dag`** | Lista todas as tabelas do dataset `DataLake` no BigQuery. |
| **`mysql_to_bigquery_only`** | Carrega dados crus do **MySQL** para o **BigQuery**. |
| **`test_env_vars_dag`** | Valida as vari√°veis de ambiente configuradas no `.env` dentro dos containers Docker, garantindo conex√µes consistentes. |
| **`medallion_struct`** | Pipeline baseada na **Medallion Architecture** (Bronze ‚Üí Silver ‚Üí Gold), garantindo governan√ßa, qualidade e integra√ß√£o entre processos upstream e downstream. |

---

## üóÇÔ∏è Estrutura de Dados Utilizada

**OLIST Dataset**  
![Diagrama Olist](img/OLIST_DIAGRAM.png)

---

## üõ†Ô∏è Tecnologias e Ferramentas

- [Apache Airflow](https://airflow.apache.org/)  
- [Google Cloud BigQuery](https://cloud.google.com/bigquery)  
- [MySQL](https://www.mysql.com/)  
- [Google Cloud SDK](https://cloud.google.com/sdk) ‚Äì autentica√ß√£o e acesso aos servi√ßos GCP  
- [Gmail API](https://developers.google.com/gmail/api) ‚Äì envio autom√°tico de notifica√ß√µes em caso de erro nas pipelines  
- [Docker & Docker Compose](https://www.docker.com/) ‚Äì conteineriza√ß√£o dos servi√ßos  
- [Git & GitHub](https://github.com/) ‚Äì controle de vers√£o  

---

## üèóÔ∏è Como Executar o Projeto

1. **Clone o reposit√≥rio**  
   ```bash
   git clone https://github.com/renan-lemes/pipelines-Airflow
   cd pipelines-Airflow
   ```

2. **Configure o arquivo `.env`** com as credenciais e vari√°veis necess√°rias:  
   ```env
   MYSQL_HOST=localhost
   MYSQL_PORT=3306
   MYSQL_USER=root
   MYSQL_PASSWORD=senha
   MYSQL_DATABASE=meu_banco

   GOOGLE_PROJECT_ID=meu-projeto
   GOOGLE_DATASET_ID=DataLake
   GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/dags/key.json

   EMAIL_SENDER=my_email@gmail.com
   EMAIL_PASSWORD=my_gmail_app_password
   EMAIL_RECEIVER=team@datateam.com
   ```

3. **Suba os containers com o Docker Compose**  
   ```bash
   docker-compose up -d
   ```

4. **Acesse o Airflow UI:** [http://localhost:8080](http://localhost:8080)  
   - Usu√°rio padr√£o: `airflow`  
   - Senha padr√£o: `airflow`  

5. **Configure as conex√µes** no Airflow (Admin ‚Üí Connections):  
   - **MySQL:** `mysql_default`  
   - **BigQuery:** `google_cloud_default`  
   - **Email:** `smtp_default` (para envio de alertas)  

6. **Ative a DAG desejada** na interface do Airflow e monitore sua execu√ß√£o.  

---

## üìß Alertas Autom√°ticos de Erros

O Airflow foi configurado para **enviar e-mails autom√°ticos via Gmail** quando uma DAG falhar.  
O alerta inclui:
- Nome da DAG e da task com erro  
- Hor√°rio da falha  
- Log resumido da execu√ß√£o  

Isso permite **resposta r√°pida** e acompanhamento proativo de falhas em produ√ß√£o.

---

## üìã Boas Pr√°ticas e Observa√ß√µes

- Mantenha logs de execu√ß√£o (quantidade de registros, timestamps, status) para acompanhamento da **qualidade e integridade dos dados**.  
- Utilize o **scheduler do Airflow** para orquestrar rotinas peri√≥dicas.  
- Cada pipeline pode ser ajustada para ambientes **dev**, **staging** e **prod** apenas alterando vari√°veis de ambiente.  
- Recomendado integrar com **GitHub Actions** para CI/CD, garantindo que atualiza√ß√µes de DAGs sejam testadas e implantadas automaticamente.  

---

---

## üß± Arquitetura Medallion

A pipeline **`medallion_struct`** segue o modelo de camadas **Bronze**, **Silver** e **Gold**:

- **Bronze:** dados crus extra√≠dos dos dados, sem transforma√ß√µes.  
- **Silver:** dados tratados e padronizados (ajuste de colunas, formata√ß√£o de tipos e valida√ß√µes).  
- **Gold:** dados agregados e prontos para visualiza√ß√£o no dashboard.

üìä **Representa√ß√£o Visual da Arquitetura:**
üí° Exemplo de execu√ß√£o no Airflow UI (DAG Medallion):
![DAG Medallion Exemplo](img/ExemploDag.png)

---

## üìä Dashboard com Dados Gold ‚Äì Looker Studio

Ap√≥s o processamento dos dados nas camadas *Bronze*, *Silver* e *Gold*, foi criado um **dashboard no Looker Studio** para an√°lise e visualiza√ß√£o dos dados refinados.  

![Dashboard](img/Dash.png)

üîó **Acesse o Dashboard:**  
[https://lookerstudio.google.com/u/0/reporting/a3eccf51-1a31-44b7-9e57-9faac8efced6/page/iURdF](https://lookerstudio.google.com/u/0/reporting/a3eccf51-1a31-44b7-9e57-9faac8efced6/page/iURdF)
 