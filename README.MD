# Airflow MySQL â†” BigQuery Pipelines

Este repositÃ³rio contÃ©m pipelines desenvolvidos em **Apache Airflow** para **extrair, transformar e carregar (ETL)** dados entre **MySQL** e **Google BigQuery**.  
Os fluxos permitem tanto a transferÃªncia de dados do MySQL para o BigQuery quanto o caminho inverso, garantindo maior controle e automaÃ§Ã£o no processo.

---

## ğŸš€ Objetivo

Substituir processos manuais ou scripts isolados de integraÃ§Ã£o de dados por pipelines orquestrados com o **Airflow**, oferecendo:  
- ğŸ”„ Flexibilidade e personalizaÃ§Ã£o.  
- ğŸ“ˆ Escalabilidade e monitoramento.  
- ğŸ› ï¸ Facilidade de manutenÃ§Ã£o e versionamento.  

---

## âš™ï¸ Funcionalidades

- ExtraÃ§Ã£o de dados a partir do **MySQL**.  
- Carregamento de dados no **BigQuery** (e vice-versa).  
- TransformaÃ§Ãµes leves e personalizadas em Python.  
- ExecuÃ§Ãµes agendadas e automÃ¡ticas via Airflow.  
- Monitoramento centralizado com logs detalhados pela **UI do Airflow**.  

---

## ğŸ“‚ Pipelines DisponÃ­veis

- **`bigquery_to_mysql_only`** â†’ Carrega dados crus do BigQuery para o MySQL.  
- **`list_bigquery_tables_dag`** â†’ Lista todas as tabelas do dataset `DataLake` no BigQuery.  
- **`mysql_to_bigquery_only`** â†’ Carrega dados crus do MySQL para o BigQuery.  
- **`test_env_vars_dag`** â†’ Valida se as variÃ¡veis de ambiente configuradas no `.env` estÃ£o corretas dentro da conteinerizaÃ§Ã£o (Docker), garantindo conexÃµes consistentes com diferentes bancos de dados.  
- **`medallion_struct`** â†’ Uma pipeline que segue a **Medallion Architecture** (Bronze â†’ Silver â†’ Gold), garantindo melhor governanÃ§a e qualidade dos dados. Esse fluxo foi projetado para lidar tanto com processos upstream quanto downstream, permitindo integraÃ§Ã£o com diferentes sistemas.
---

## ğŸ—‚ï¸ Estrutura de Dados Utilizada

- **OLIST**  
![Diagrama Olist](img/OLIST_DIAGRAM.png)

---

## ğŸ› ï¸ Tecnologias e Ferramentas

- [Apache Airflow](https://airflow.apache.org/)  
- [Google Cloud BigQuery](https://cloud.google.com/bigquery)  
- [MySQL](https://www.mysql.com/)  
- [Google Cloud SDK](https://cloud.google.com/sdk) â€“ autenticaÃ§Ã£o e acesso aos serviÃ§os GCP  

---

## ğŸ—ï¸ Como Rodar o Projeto

1. **Clone o repositÃ³rio**  
   ```bash
   git clone https://github.com/renan-lemes/pipelines-Airflow
   cd pipelines-Airflow
   ```

2. **Crie e configure o arquivo `.env`** com as credenciais necessÃ¡rias (MySQL, BigQuery e variÃ¡veis do Airflow):  
   ```env
   MYSQL_HOST=localhost
   MYSQL_PORT=3306
   MYSQL_USER=root
   MYSQL_PASSWORD=senha
   MYSQL_DATABASE=meu_banco

   GOOGLE_PROJECT_ID=meu-projeto
   GOOGLE_DATASET_ID=DataLake
   GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/dags/key.json
   ```

3. **Suba os containers do Airflow com Docker Compose**  
   ```bash
   docker-compose up -d
   ```

4. **Acesse o Airflow UI** em [http://localhost:8080](http://localhost:8080).  
   - UsuÃ¡rio padrÃ£o: `airflow`  
   - Senha padrÃ£o: `airflow`  

5. **Configure as conexÃµes no Airflow UI** (Admin â†’ Connections):  
   - **MySQL** â†’ `mysql_default`  
   - **BigQuery** â†’ `google_cloud_default`  

6. **Ative a DAG desejada** na interface do Airflow e monitore a execuÃ§Ã£o.  

---

## ğŸ“Œ ObservaÃ§Ãµes
  
- Ã‰ recomendÃ¡vel manter logs de execuÃ§Ã£o (quantidade de registros carregados, timestamp da atualizaÃ§Ã£o) para monitoramento da qualidade dos dados.  
